{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning & EDA\n",
    "*Author: [Douglas Strodtman](http://linkedin.com/in/dstrodtman/)*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning and exploratory data analysis often go hand in hand. \n",
    "- Without examining our data, it's difficult to know whether or not there are errors in it. \n",
    "- Without cleaning our data, our aggregate statistics may be skewed by errant data.\n",
    "The interplay of these processes is often very cyclical. For a data science workflow, these steps are essential to help us understand the nature of our data and ensure that we haven't injected or propogated unnecessary noise to our modeling algorithm. Oftentimes we will find ourselves circling back to data cleaning and EDA after modeling when we are dissatisfied with results.\n",
    "\n",
    "**No matter your goals working with data, becoming proficient with cleaning and EDA is amongst the most important skills you can learn.**\n",
    "\n",
    "## Skills Covered\n",
    "1. Module import\n",
    "1. Data import\n",
    "1. Previewing Data\n",
    "1. Renaming Columns\n",
    "1. Masking\n",
    "1. Reindexing\n",
    "1. Summary Statistics\n",
    "1. `groupby` and Aggregation\n",
    "1. Pivot Tables\n",
    "1. Missing data\n",
    "    - Finding missing values\n",
    "    - Imputing missing data\n",
    "1. Data export\n",
    "\n",
    "## Key Objectives\n",
    "\n",
    "Our walkthrough will focus on data from the years 2017 and 2018. By the end of this lesson, you'll be able to answer the following questions (which will be the focus of the accompanying lab):\n",
    "\n",
    "- Which department had the most line item entries each year?\n",
    "- Which department had the highest total expenditures each year?\n",
    "- Which fund had the highest budget allocation each year?\n",
    "- What percentage of money from the general fund was allocated to different departments each year?\n",
    "- Which departments saw the largest budget increase and decrease from 2017 to 2018?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module Import\n",
    "Start off by importing pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import\n",
    "Load the full data. Use a relative path so that your code will be robust.\n",
    "\n",
    "To see all the data files that were included with this lesson, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import this to the variable `all_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview Data\n",
    "Look at the first 5 rows of your data to see how it loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While our default options appear to have successfully loaded the data, we have column names that are all caps and contain spaces. Let's fix this before moving forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renaming Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As long as our column names are only letters, numbers, and underscores, we can also use a dot notation to access Series. In addition, this format will work accross almost all parts of your data workflow, and is especially friendly to SQL.\n",
    "\n",
    "Let's start by looking at all of our columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're aiming for `snake_case` here, which means we'll want only lowercase letters and underscores.\n",
    "\n",
    "Let's start by just saving our lowercase strings to a new variable, `columns_clean`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a next step, let's just replace the hyphens, overwriting our variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can replace our spaces with underscores as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we've maintained the order of our columns, we can safely overwrite the original columns in our DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview the first 3 rows to see that this worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking\n",
    "\n",
    "We're only interested in data from 2017 and 2018. Let's set up a unique mask for each of these years.\n",
    "\n",
    "To do this, we'll just do a check for equality on our `budget_fiscal_year`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now put these masks back into our DataFrame to look at only those rows for each year. Let's do this for each year and check the `shape` attribute so we can see how many rows we're selecting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the bitwise `or` operator `|` to select all those rows where either of these conditions are true. The number of rows here should equal 7246."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we know that this is the data we wish to work with for the remainder of our exploration, let's save this out to a new DataFrame `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's take a sample of 10 rows to do a quick check that we haven't included any data from other years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reset Index\n",
    "\n",
    "You'll note in the preview above that our indices are quite high. This index is not especially informative (it was generated by Pandas automatically upon import).\n",
    "\n",
    "Personally, when my index doesn't correspond to a primary key, I prefer to work with a serial index starting at 0.\n",
    "\n",
    "This method is also helpful for returning columns that you've used in a `groupby` statement back into your main DataFrame (more on this later).\n",
    "\n",
    "Make sure to set the argument `drop=True` if you want to discard your old index (here, we desire this functionality).\n",
    "\n",
    "In addition, once you've checked that your code is working, you should set `inplace=True` to persist these changes in your `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Stats\n",
    "\n",
    "We've already looked at the shape of our data, but let's check out our `info` to see the types and make note of any missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can look at our overall numeric summary statistics. (Don't forget to transpose to make these easy to read)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there anything of value you note here? Do these numbers provide insight into any of the questions we originally sought to answers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Be careful!\n",
    "\n",
    "Summary statistics can be misleading. The canonical example of this is [Ancombe's Quartet](https://en.wikipedia.org/wiki/Anscombe%27s_quartet)\n",
    "\n",
    "**NOTE**: The following is taken directly from Wikipedia:\n",
    "\n",
    "![Ancombe's Quartet](../images/anscombes_quartet.png)\n",
    "\n",
    "| Property | Value | Accuracy |\n",
    "| --- | --- | --- |\n",
    "| Mean of $x$ | 9 | exact |\n",
    "| Sample variance of $x$ | 11 | exact |\n",
    "| Mean of $y$ | 7.50 | to 2 decimal places |\n",
    "| Sample variance of $y$ | 4.125 | Â±0.003 |\n",
    "| Correlation between $x$ and $y$ | 0.816 | to 3 decimal places |\n",
    "| Linear regression line | $y$&nbsp;=&nbsp;3.00&nbsp;+&nbsp;0.500$x$ | to 2 and 3 decimal places, respectively |\n",
    "| Coefficient of determination of the linear regression | 0.67 | to 2 decimal places |\n",
    "\n",
    "**While we can generally rely on summary statistics to give a good overview of our data, we should always do further data exploration before using summary statistics to support our claims.** Next week's lesson will focus exclusively on visualizing our data; data visualization is useful both for conveying findings and visually confirmation of mathematically-derived findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `groupby` and Aggregation\n",
    "\n",
    "We're not actually interested in aggregate statistics calculated over the entire column. Rather, we want to identify groups.\n",
    "\n",
    "When using `groupby`, you'll need to also apply an aggregation method. Some useful aggregation methods include:\n",
    "\n",
    "| method | function |\n",
    "| --- | --- |\n",
    "| `.count` | Returns the count of total rows that have been grouped together. |\n",
    "| `.sum` | Returns the sum of all the rows in each group. |\n",
    "| `.mean` | Returns the average of all the rows in each group. |\n",
    "\n",
    "Let's start by just grouping by our `budget_fiscal_year` and calculating the mean. Transpose the result for easier interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use `value_counts` and `describe` with `groupby`, but I'd recommend you limit these to a single column.\n",
    "\n",
    "Let's use `describe` on our `total_budget` grouped by `budget_fiscal_year`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All let's look at the `value_counts` of our `department_name` when grouped by year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's difficult to garner any insights from this preview.\n",
    "\n",
    "## `.pivot_table`\n",
    "\n",
    "Instead, we'll create a [pivot table](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.pivot_table.html).\n",
    "\n",
    "In my experience, power users of Excel think in pivot tables, whereas folks coming to data from a more programmatic background can struggle with the concept.\n",
    "\n",
    "I think of it as allowing you to find the unique intersections of two different GROUP BY statements, and then identify an additional column to aggregate over.\n",
    "\n",
    "Here's a break down of the arguments:\n",
    "\n",
    "| arg | function |\n",
    "| --- | --- |\n",
    "| `values` | A column that will be aggregated |\n",
    "| `index` | A column or list of columns; unique values will become the index of the resultant DataFrame |\n",
    "| `columns` | A column or list of columns; unique values will become the columns of the resultant DataFrame |\n",
    "| `aggfunc` | A aggregate function or list of aggregate functions that will be applied to the specified `values` column |\n",
    "\n",
    "**Note**: There are additional layers of complex functionality available in this method, which is extremely powerful for data exploration.\n",
    "\n",
    "Here, we'll return the `count` and `sum` of the `total_budget` with `department_name` as our index and `budget_fiscal_year` as our columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're getting very close to being able to having all the tools we need to answer the questions we posed at the beginning of the lesson. However, there's still an elephant in the room...\n",
    "\n",
    "## Missing Data\n",
    "\n",
    "What _should_ we do about the missing values in our data?\n",
    "\n",
    "This is a difficult question to answer. Unless you are reasonably confident that you can find the true value for a missing data point, you should always be careful when imputing a value. Without going too far into this, a few concerns with data imputation include:\n",
    "\n",
    "1. Changes to distributions\n",
    "1. Reduction in variance\n",
    "1. Obfuscation of meaningful nulls\n",
    "1. Data is \"made up\"\n",
    "\n",
    "Many modeling techniques will require that you deal with all null values before moving forward, so there may be times that you have to impute missing values. A few common approaches include using:\n",
    "\n",
    "1. The mean, median, or mode\n",
    "1. A random value selected from the distribution of values in the sample\n",
    "1. A placeholder to indicate missingness (e.g. -1, 999999, '?')\n",
    "\n",
    "Each of these is imperfect, and in all cases, it's **imperative** to clearly indicate that you've edited missing values if you're going to store this data for later analysis. (Imagine coming across a dataset with a mostly normal distribution but a huge spike of values right at the median. How would you implicitly know whether this data were real or the result of data cleaning?)\n",
    "\n",
    "That being said, let's go ahead look at the total number of missing values in each column to see if we can derive a plan of attack.\n",
    "\n",
    "The `isna` method returns a boolean list that we can `sum` to get these counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I propose that the missingness represented by our 3 columns is unlikely to be random. Let's examine each column indepedently before making any decisions.\n",
    "\n",
    "### Account Name\n",
    "\n",
    "We'll start with `account_name`, which has the fewest nulls.\n",
    "\n",
    "Let's begin by creating a mask of each row that has a missing value here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use this to review the values present in these rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My thought would be to see whether or not we can identify with cetainty the `account_name` by looking at other rows with the same `account` code.\n",
    "\n",
    "We can use `isin` to find the rows that match here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By selecting only those columns we're interested in and sorting them, we can quickly see that in these 4 cases, it's probably safe to impute the account names used in other instances of the account code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While in this case our data were small enoug that we could visually review this, let's work out a way to do this programmatically.\n",
    "\n",
    "The `dropna` method will, by default, drop rows that contain any nulls. **These changes will only persist if you use the `inplace=True` keyword argument.** Let's start building up our argument by again applying our mask, selecting our columns of interest, and dropping those rows containing nulls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get only those rows that are distinct, we can use `drop_duplicates`. Again, these changes won't persist unless we use the `inplace=True` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can clearly see that we have only one `account_name` for each `account`.\n",
    "\n",
    "Calling `values` on the previous command will return an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which we can cast as a dictionary to make our `account` the keys and the `account_name` the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we `map` this back our `account` column for those missing rows, we'll return our `account_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which we can assign back to our DataFrame with `.loc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Expenditures\n",
    "\n",
    "If you recall from our first lab, the missingness in our total expenditures can't be easily calculated.\n",
    "\n",
    "Let's create a `exp_null` mask so we can explore this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we know we have a lot of rows here, let's just look at the first 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These first 10 rows have many zero values for the numeric columns, and also have identical values for the `budget_change_amount`, `total_budget`, and `budget_uncommitted_amount`. Let's explore how commonly these observations are true in the rest of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for comparison, let's look at how commonly this amount is zero when expenditures aren't null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that visual inspection of a sample led us to a spurious hypothesis. Indeed, we can remember from our earlier investigation of our summary statistics that many of our numeric fields have many 0 values. Let's abandon further investigation of zeroes for now.\n",
    "\n",
    "Instead, let's build logic to investigate `budget_change_amount`, `total_budget`, and `budget_uncommitted_amount`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that not all of our observations have 0 for the `adopted_budget_amount`, it follows that `budget_change_amount` and `total_budget` won't be equal in many of our rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do see, however, that our uncommited budget is equal to our total budget in most of our data. If you recall, we should actually also be including the `encumbrance_amount` here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very nearly every row. (We could push further and check for floating point errors in our calculations, but we'll skip this for now).\n",
    "\n",
    "Based on our observations, do you think it's safe to impute `0` into our `total_expenditures` column for missing values?\n",
    "\n",
    "Remember, most of our aggregate calculations in Pandas will ignore null values by default. If we impute something (whether it's zero, the mean, a numeric placeholder, or a random value), these values will factor into any future summary statistics. **I would recommend, whenever possible, that you avoid imputation until you have completed all of your EDA.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Account Group Name\n",
    "\n",
    "This column had the highest number of nulls in our entire dataset. Let's look at the percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that many nulls, let's see what our `value_counts` are for this field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nature and distribution of these labels suggests that they are optional tags that provide additional context to line items. As such, the best we can likely due in order to eliminate the null values, is to use a filler string like `\"UNSPECIFIED\"`.\n",
    "\n",
    "We can use `fillna` with the argument `inplace=True` to persist these changes to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Export\n",
    "\n",
    "Saving data in Pandas is just as easy as loading data. Here, we'll save our data back to our data directory with the name `clean1718.csv` using the `to_csv` method. Because our numeric index is not meaningful here, we can pass the keyword argument `index=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
